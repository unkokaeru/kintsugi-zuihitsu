{"path":"attached/files/Session_9_Ordinary_Differential_Equations.pdf","text":"NUMERICAL METHODS WEEK 9NUMERICAL METHODS WEEK 9 ODES AND DECOMPOSITIONSODES AND DECOMPOSITIONS Learning outcomes: Practice using \u0000nite differences to solve end point differential equations. Look at decompositions of matrix eqautions and how they can be used to solve linear equations. MATT WATKINS MWATKINS@LINCOLN.AC.UKMATT WATKINS MWATKINS@LINCOLN.AC.UK DIFFERENTIAL EQUATIONSDIFFERENTIAL EQUATIONS Some differential equations can be solved numerically as follows (more details next term). We can approximate a function on a grid of points that are spaced by . We can approximate derivatives of by \u0000nite differences In matrix form we can write this as y = f(x) n Δx y = = yd2 dx2 ∣ ∣i ( − )/Δx − ( − )/Δxyi+1 yi yi yi−1 Δx − 2 +yi+1 yi yi−1 Δx2 1 Δx2 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ −2 1 0 0 0 0 0 0 −2 ⋱ 0 0 0 0 0 1 ⋱ 1 0 0 0 0 0 ⋱ −2 ⋱ 0 0 0 0 0 1 ⋱ 0 0 0 0 0 0 ⋱ 1 0 0 0 0 0 0 −2 0 0 0 0 0 0 1 −2 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ y0 y1 ⋮ yi ⋮ yn−2 yn−1 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ FINITE DIFFERENCESFINITE DIFFERENCES We change our continuous independent variable into a discrete set of equally spaced points Add point The gap between the points, , is given by the range of the interval divided by the number of points -1 , i.e. x nJSXGraph v1.4.6 Copyright (C) see https://jsxgraph.org 0.5 1 1.5 2 2.5 3 3.5−0.5 0.5 1 1.5 2 2.5 −0.5 −1 −1.5 −2 −2.5 Δx = +xi xmin i( − )xmax xmin (n − 1) x 0 x n-1x1 x2 x3 x4 x5 x6 x7 x8 x9 dx – o + ← ↓ ↑ → DIFFERENTIAL EQUATIONSDIFFERENTIAL EQUATIONS Some differential equations can be solved numerically as follows (more details next term). We can approximate a function on a grid of points that are spaced by . We can approximate derivatives of by \u0000nite differences and then using this approximation again we get: This is known as central differences - you can also work with forward or backward differences to get different approximations. y(x) = f(x) n Δx y( )xi ≈ dy( )xi dx ( − )yi+Δx/2 yi−Δx/2 Δx ≈ ( − ) /Δx = y( )d2 xi dx2 dy( + Δx/2)xi dx dy( − Δx/2)xi dx ( − )/Δx − ( − )/Δxyi+1 yi yi yi−1 Δx = − 2 +yi+1 yi yi−1 Δx2 In matrix form we can write this as If we multiply out the 2 nd row we get and the 3 rd row gives ... 1 Δx2 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ −2 1 0 0 0 0 0 0 −2 ⋱ 0 0 0 0 0 1 ⋱ 1 0 0 0 0 0 ⋱ −2 ⋱ 0 0 0 0 0 1 ⋱ 0 0 0 0 0 0 ⋱ 1 0 0 0 0 0 0 −2 0 0 0 0 0 0 1 −2 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ y0 y1 ⋮ yi ⋮ yn−2 yn−1 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ( − 2 + ) ≈ 1 Δx2 y0 y1 y2 y( )d2 x1 dx2 ( − 2 + ) ≈ 1 Δx2 y1 y2 y3 y( )d2 x2 dx2 DIFFERENTIAL EQUATIONDIFFERENTIAL EQUATION We can write where contains everything that is not a function of as where and are the values of at the left and right boundaries. This is a set of linear equations that can be solved using Gauss Elimination, or other methods such as LU decomposition (see later). Note we multiplied through by to avoid dividing by a very small number. = g(x) y(x)d2 dx2 g(x) y = ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ −2 1 0 0 0 0 0 0 −2 ⋱ 0 0 0 0 0 1 ⋱ 1 0 0 0 0 0 ⋱ −2 ⋱ 0 0 0 0 0 1 ⋱ 0 0 0 0 0 0 ⋱ 1 0 0 0 0 0 0 −2 0 0 0 0 0 0 1 −2 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ y0 y1 ⋮ yi ⋮ yn−2 yn−1 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ −2yL Δg1 x2 ⋮ Δgi x2 ⋮ Δgn−2 x2 −2yR ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ yL yR y Δx2 Multiplying out the 2 nd row and dividing by we get and for the third row the same for other rows. Again, I multiplied through by because this will be small and can cause numerical problems if you divide numbers by it. = ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ −2 1 0 0 0 0 0 0 −2 ⋱ 0 0 0 0 0 1 ⋱ 1 0 0 0 0 0 ⋱ −2 ⋱ 0 0 0 0 0 1 ⋱ 0 0 0 0 0 0 ⋱ 1 0 0 0 0 0 0 −2 0 0 0 0 0 0 1 −2 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ y0 y1 ⋮ yi ⋮ yn−2 yn−1 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ −2yL Δg1 x2 ⋮ Δgi x2 ⋮ Δgn−2 x2 −2yR ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ Δx2 ( − 2 + ) ≈ = g( ) 1 Δx2 y0 y1 y2 y( )d2 x1 dx2 x1 ( − 2 + ) ≈ = g( ) 1 Δx2 y1 y2 y3 y( )d2 x2 dx2 x2 Δx2 BOUNDARY CONDITIONSBOUNDARY CONDITIONS Note that the \u0000rst and last rows are different. They are completely decoupled from the others and just read and i.e. that the boundary conditions are satis\u0000ed at the edges of the range we are solving over. The convention of putting -2 on the LHS is only to keep the matrix looking pretty, you could put 1 s and just yL and yR on the RHS. The boundary values are \u0000xed, but they effect the other points because they appear in other rows −2 = −2y0 yL −2 = −2yN−1 yR ( − 2 + ) ≈ = g( ) 1 Δx2 yL y1 y2 y( )d2 x1 dx2 x1 EXAMPLE: EXAMPLE: Find a solution for in the interval [0 ,1 ] when: with If we discretize into points we get get our points . We can write our equations at each point as: = x yd2 dx2 y g(x) = x, = 0.2, = 1.5yL yR n = 10, 100, 500 n = 0 +xi i(1−0) (n−1) = ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ −2 1 0 0 0 0 0 0 −2 ⋱ 0 0 0 0 0 1 ⋱ 1 0 0 0 0 0 ⋱ −2 ⋱ 0 0 0 0 0 1 ⋱ 0 0 0 0 0 0 ⋱ 1 0 0 0 0 0 0 −2 0 0 0 0 0 0 1 −2 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ y0 y1 ⋮ yi ⋮ yn−2 yn−1 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ −2 × 0.2 Δx1 x2 ⋮ Δxi x2 ⋮ Δxn−2 x2 −2 × 1.5 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ EXAMPLE EXAMPLE Solve when , all other and in the interval If we discretize into points we get get our points . We can write our equations at each point as: = g(x) yd2 dx2 g( ) = 1/Δxxn/2 = 0gi = 0, = 0yL yR [−5, 5] n = −5 +xi i(5−−5) (n−1) = ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ −2 1 0 0 0 0 0 0 −2 ⋱ 0 0 0 0 0 1 ⋱ 1 0 0 0 0 0 ⋱ −2 ⋱ 0 0 0 0 0 1 ⋱ 0 0 0 0 0 0 ⋱ 1 0 0 0 0 0 0 −2 0 0 0 0 0 0 1 −2 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ y0 y1 ⋮ yn/2 ⋮ yn−2 yn−1 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ −2 × 0 0 × Δx2 ⋮ Δ 1 Δx x2 ⋮ 0 × Δx2 −2 × 0 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ EXAMPLE EXAMPLE Solve when , in the interval To get our linear equation we must have , in this case the LHS must approximate + ky = C yd2 dx2 + ky = g(x) yd2 dx2 g(x) = C = 5, = 0yL yR [−3, 3] D(x)y(x) = g(x) ( + k)y d2 dx2 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ −2 1 0 0 0 0 0 0 −2 + kΔx2 ⋱ 0 0 0 0 0 1 ⋱ 1 0 0 0 0 0 ⋱ −2 + kΔx2 ⋱ 0 0 0 0 0 1 ⋱ 0 0 0 0 0 0 ⋱ 1 0 0 0 0 0 0 −2 + kΔx2 0 0 0 0 0 0 1 −2 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ y0 y1 ⋮ yi ⋮ yn−2 yn−1 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ TASK SOLVE THE QUESTION IN THE WEEK 9 ONLINE TEST.TASK SOLVE THE QUESTION IN THE WEEK 9 ONLINE TEST. Note, you will need to think how to include the \u0000rst derivative of y into the LHS too. LU DECOMPOSITIONLU DECOMPOSITION Again we want to solve Suppose we can rewrite this as which looks similar to Gauss elmination. In matrix notation . Now, assume there is a lower diagonal matrix with '1 's on the diagonal that has the property that, premultiplying by we get Ax - b = 0 − = 0 ⎛ ⎝ ⎜ u00 0 0 u01 u11 0 u02 u12 u22 ⎞ ⎠ ⎟ ⎛ ⎝ ⎜ x0 x1 x2 ⎞ ⎠ ⎟ ⎛ ⎝ ⎜ d0 d1 d2 ⎞ ⎠ ⎟ Ux - d = 0 L ⎛ ⎝ ⎜ 1 l10 l20 0 1 l21 0 0 1 ⎞ ⎠ ⎟ L LUx − Ld = Ax − b LU DECOMPOSITIONLU DECOMPOSITION To ensure that we require that and LUx − Ld = Ax − b LU = A Ld = b GAUSSIAN ELIMINATIONGAUSSIAN ELIMINATION TRIANGULARIZATIONTRIANGULARIZATION When we do the Gauss elimination method, we actually \u0000nd all the elements of , we just need to store them. It is the inverse of the matrix we would need to multiply by to get the correct RHS in the Gauss elmination method. Let us take an initial augmented matrix, and record the values that we would scale the matrix by if it were there: pivoting around row 0 , we remove all entries below the diagonal entry in column 0 , doing this we scaled the matrix by the ratios shown on the right L b b ⎛ ⎝ ⎜ ⎜ ⎜ a00 a10 a20 a30 a01 a11 a21 a31 a02 a12 a22 a32 a03 a13 a23 a33 ⎞ ⎠ ⎟ ⎟ ⎟ b ⎛ ⎝ ⎜ ⎜⎜ ⎜ a00 0 0 0 a01 a′ 11 a′ 21 a′ 31 a02 a′ 12 a′ 22 a′ 32 a03 a′ 13 a′ 23 a′ 33 ⎞ ⎠ ⎟ ⎟⎟ ⎟ ⎛ ⎝ ⎜⎜ ⎜⎜⎜ 1 a10 a00 a20 a00 a30 a00 0 0 0 ⎞ ⎠ ⎟⎟ ⎟⎟⎟ Matrix after pivoting around row 0 Then pivoting around row 1 we remove elements below the diagonal in column 1 , and subtract multiples of the 2 nd row of as shown in the right hand matrix pivoting around row 2 ⎛ ⎝ ⎜ ⎜⎜ ⎜ a00 0 0 0 a01 a′ 11 a′ 21 a′ 31 a02 a′ 12 a′ 22 a′ 32 a03 a′ 13 a′ 23 a′ 33 ⎞ ⎠ ⎟ ⎟⎟ ⎟ ⎛ ⎝ ⎜⎜ ⎜⎜⎜ 1 a10 a00 a20 a00 a30 a00 0 0 0 ⎞ ⎠ ⎟⎟ ⎟⎟⎟ b ⎛ ⎝ ⎜ ⎜⎜ ⎜ a00 0 0 0 a01 a′ 11 0 0 a02 a′ 12 a′′ 22 a′′ 32 a03 a′ 13 a′′ 23 a′′ 33 ⎞ ⎠ ⎟ ⎟⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ 1 a10 a00 a20 a00 a30 a00 1 a′ 21 a′ 10 a′ 31 a′ 10 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜⎜ ⎜ a00 0 0 0 a01 a′ 11 0 0 a02 a′ 12 a′′ 22 0 a03 a′ 13 a′′ 23 a′′′ 33 ⎞ ⎠ ⎟ ⎟⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ 1 a10 a00 a20 a00 a30 a00 1 a′ 21 a′ 11 a′ 31 a′ 11 1 a′′ 32 a′′ 22 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ we had got to if we \u0000ll in the rest of the matrix on the right we have ⎛ ⎝ ⎜ ⎜⎜ ⎜ a00 0 0 0 a01 a′ 11 0 0 a02 a′ 12 a′′ 22 0 a03 a′ 13 a′′ 23 a′′′ 33 ⎞ ⎠ ⎟ ⎟⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ 1 a10 a00 a20 a00 a30 a00 1 a′ 21 a′ 11 a′ 31 a′ 11 1 a′′ 32 a′′ 22 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜⎜ ⎜ a00 0 0 0 a01 a′ 11 0 0 a02 a′ 12 a′′ 22 0 a03 a′ 13 a′′ 23 a′′′ 33 ⎞ ⎠ ⎟ ⎟⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ 1 a10 a00 a20 a00 a30 a00 0 1 a′ 21 a′ 11 a′ 31 a′ 11 0 0 1 a′′ 32 a′′ 22 0 0 0 1 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ We can show for a real system that this new matrix is and L = ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ 1 a10 a00 a20 a00 a30 a00 0 1 a′ 21 a′ 11 a′ 31 a′ 11 0 0 1 a′′ 32 a′′ 22 0 0 0 1 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜⎜ ⎜ a00 0 0 0 a01 a′ 11 0 0 a02 a′ 12 a′′ 22 0 a03 a′ 13 a′′ 23 a′′′ 33 ⎞ ⎠ ⎟ ⎟⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ a00 a10 a20 a30 a01 a11 a21 a31 a02 a12 a22 a32 a03 a13 a23 a33 ⎞ ⎠ ⎟ ⎟ ⎟ LU = A Ld = b v o i d g a u s s _ e l i m ( s t d : : o f s t r e a m & m y o u t , d o u b l e ( & a ) [ r o w s ] [ c o l s ] ) { d o u b l e c o e f f ; d o u b l e x [ r o w s ] ; f o r ( i n t i = 0 ; i < r o w s - 1 ; i + + ) { f o r ( i n t j = i + 1 ; j < r o w s ; j + + ) { c o e f f = a [ j ] [ i ] / a [ i ] [ i ] ; f o r ( i n t k = i ; k < c o l s ; k + + ) { a [ j ] [ k ] - = a [ i ] [ k ] * c o e f f ; } } m y o u t < < \" \\ n p i v o t i n g a r o u n d r o w \" < < i < < \" \\ n \\ n \" ; o u t p u t _ m a t r i x ( m y o u t , a ) ; } } h i g h l i g h t : h l j s c s The entries in are just what I called `coeff` in the code So only very minor changes in the Gauss elimination code are needed. Note, I have stored the nondiagonal elements of in what would be the zero elements of the row echelon matrix. L v o i d g a u s s _ e l i m ( s t d : : o f s t r e a m & m y o u t , d o u b l e ( & a ) [ r o w s ] [ c o l s ] ) { d o u b l e c o e f f ; d o u b l e x [ r o w s ] ; f o r ( i n t i = 0 ; i < r o w s - 1 ; i + + ) { f o r ( i n t j = i + 1 ; j < r o w s ; j + + ) { c o e f f = a [ j ] [ i ] / a [ i ] [ i ] ; a [ j ] [ i ] = c o e f f ; f o r ( i n t k = i + 1 ; k < c o l s ; k + + ) { a [ j ] [ k ] - = a [ i ] [ k ] * c o e f f ; } } } } h i g h l i g h t : h l j s c s L LU DECOMPOSITIONLU DECOMPOSITION The major advantage of LU decompostion is we calculate and once, then we can easily \u0000nd for any Having we can solve for by forward substitution i.e. multiplying out the equations starting from the top row. Then, having constructed for the given we continue exactly like in Gauss Elimination using back substitution to \u0000nd L U x b L Ld = b d = ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ 1 a10 a00 a20 a00 a30 a00 0 1 a′ 21 a′ 11 a′ 31 a′ 11 0 0 1 a′′ 32 a′′ 22 0 0 0 1 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ d0 d1 d2 d3 ⎞ ⎠ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ b0 b1 b2 b3 ⎞ ⎠ ⎟ ⎟ ⎟ d b x = ⎛ ⎝ ⎜ ⎜⎜ ⎜ a00 0 0 0 a01 a′ 11 0 0 a02 a′ 12 a′′ 22 0 a03 a′ 13 a′′ 23 a′′′ 33 ⎞ ⎠ ⎟ ⎟⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ x0 x1 x2 x3 ⎞ ⎠ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ d0 d1 d2 d3 ⎞ ⎠ ⎟ ⎟ ⎟ This is what the following call does in Eigen # i n c l u d e # i n c l u d e u s i n g n a m e s p a c e s t d ; u s i n g n a m e s p a c e E i g e n ; i n t m a i n ( ) { M a t r i x 3 f A ; V e c t o r 3 f b ; A < < 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 1 0 ; b < < 3 , 3 , 4 ; c o u t < < \" H e r e i s t h e m a t r i x A : \\ n \" < < A < < e n d l ; c o u t < < \" H e r e i s t h e v e c t o r b : \\ n \" < < b < < e n d l ; V e c t o r 3 f x = A . f u l l P i v L u ( ) . s o l v e ( b ) ; c o u t < < \" T h e s o l u t i o n i s : \\ n \" < < x < < e n d l ; } h i g h l i g h t : h l j s c p p LU DECOMPOSITIONLU DECOMPOSITION We can use LU decomposition to \u0000nd the inverse of a matrix, by setting to the columns of the identity matrix, . For example we get the \u0000rst column of by using i.e. multiplying out the equations starting from the top row. Then, having constructed for the given we continue exactly like in Gauss Elimination using back substitution to \u0000nd The we build the other columns of from the other unit vectors. b I A −1 = (1, 0, 0, 0)bT = ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ 1 a10 a00 a20 a00 a30 a00 0 1 a′ 21 a′ 11 a′ 31 a′ 11 0 0 1 a′′ 32 a′′ 22 0 0 0 1 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ d0 d1 d2 d3 ⎞ ⎠ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ 1 0 0 0 ⎞ ⎠ ⎟ ⎟ ⎟ d b x = ⎛ ⎝ ⎜ ⎜⎜ ⎜ a00 0 0 0 a01 a′ 11 0 0 a02 a′ 12 a′′ 22 0 a03 a′ 13 a′′ 23 a′′′ 33 ⎞ ⎠ ⎟ ⎟⎟ ⎟ ⎛ ⎝ ⎜⎜⎜⎜⎜ A −1 00 A −1 10 A −1 20 A −1 30 ⎞ ⎠ ⎟⎟⎟⎟⎟ ⎛ ⎝ ⎜ ⎜ ⎜ d0 d1 d2 d3 ⎞ ⎠ ⎟ ⎟ ⎟ A −1 If you want to understand the LU decomposition clearly, adapt a gauss-elim type code to also perform LU decomposition - Find the inverse of the matrix using LU decomposition Check your solution is correct by comparison to Gauss-Jordan or Eigen / numpy libraries. Note you can also get the determinant from LU decomposition. ⎛ ⎝ ⎜ ⎜ ⎜ 2 1 3 1 2 3 1 3 4 2 3 4 −2 4 1 2 ⎞ ⎠ ⎟ ⎟ ⎟ QR DECOMPOSITIONQR DECOMPOSITION WHY A NOTHER D ECOMPOSITION???WHY A NOTHER D ECOMPOSITION??? This decomposition is part of the 'QR' algorithm to \u0000nd the eigenvalues and vectors of a matrix. Any real square matrix A may be decomposed as where is an orthogonal matrix (its columns are orthogonal unit vectors meaning ) is an upper triangular matrix (also called right triangular matrix). If is invertible, then the factorization is unique if we require the diagonal elements of to be positive. A = QR, Q Q = IQt R A R QR DECOMPOSITIONQR DECOMPOSITION GRAM–SCHMIDT O RTHOGONALISATION.GRAM–SCHMIDT O RTHOGONALISATION. You may remember this from linear algebra - we build an orthogonal basis by sucessively projecting out previous basis vectors We can scan across the columns of our matrix and remove the components of any previous column by subtracting the dot product. Take an initial matrix First we'll make an orthogonal matrix , we'll keep the \u0000rst column. To get the 2 nd column orthogonal, we subtract the projection of the second column onto the \u0000rst column from the 2 nd column. A A A = ⎛ ⎝ ⎜ 12 6 −4 −51 167 24 4 −68 −41 ⎞ ⎠ ⎟ U U = ⎛ ⎝ ⎜ 12 6 −4 ? ? ? ? ? ? ⎞ ⎠ ⎟ To get the 2 nd column orthogonal, we subtract the projection of the second column onto the \u0000rst column from the 2 nd column. where the projection onto the \u0000rst column is given by so U = ⎛ ⎝ ⎜⎜ 12 6 −4 −51 − projcol1 167 − projcol1 24 − projcol1 ? ? ? ⎞ ⎠ ⎟⎟ projcol1 = ⟨ ⋅ ⟩col2 col1 ||co ||l1 col_1 ||co ||l1 = ⟨(−51, 167, 24) ⋅ ⟩ (12, 6, 4) ⎛ ⎝ ⎜ 12 6 4 ⎞ ⎠ ⎟ 1 ( + + )12 2 62 42 = (12, 6, 4) = (18, 9, −6) 294 196 U = = ⎛ ⎝ ⎜ 12 6 −4 −51 − 18 167 − 9 24 − −6 ? ? ? ⎞ ⎠ ⎟ ⎛ ⎝ ⎜ 12 6 −4 −69 158 30 ? ? ? ⎞ ⎠ ⎟ Two get the 3 rd column orthogonal, we subtract the projection of the third column onto the \u0000rst two columns. Now we make each column a unit vector to get the orthogonal matrix U = ⎛ ⎝ ⎜⎜ 12 6 −4 −69 158 30 4 − ( + )projcol1 projcol2 −68 − ( + )projcol1 projcol2 −41 − ( + )projcol1 projcol2 ⎞ ⎠ ⎟⎟ = ⎛ ⎝ ⎜ 12 6 −4 −69 158 30 −58/5 6/5 −33 ⎞ ⎠ ⎟ Q Q = ( ) = . u1 ∥ ∥u1 u2 ∥ ∥u2 u3 ∥ ∥u3 ⎛ ⎝ ⎜ 6/7 3/7 −2/7 −69/175 158/175 6/35 −58/175 6/175 −33/35 ⎞ ⎠ ⎟ Remember, we wanted if we premultiply by we get but for an orthogonal matrix so we have and we can \u0000nd my mulitplying our original matrix and the (transposed) orthogonal matrix we just found. A = QR, QT A = QR,QT QT Q = IQT A = R,QT R A .R = A =Q T ⎛ ⎝ ⎜ 14 0 0 21 175 0 −14 −70 35 ⎞ ⎠ ⎟ QR ALGORITHMQR ALGORITHM It can be shown that the sequence converges to a upper triangular matrix with the eigenvalues of as its diagonal entries. In the sequence and are formed by the QR decomposition of This algorithm needs modi\u0000cations to be generally ef\u0000cient, but gives the general shape of real eigenvalue \u0000nding routines. If we have an eigenvalue we can \u0000nd the corresponding eigenvector using Gauss Elimination. A 0 A k+1 = A = RkQk A Qk Rk A k =A k QkRk Find the eigenvalues of the following matrices using the QR decomposition code on Bb (or write your own of course): . . Check that your eigenvalues are eigenvalues by running Gauss elimination on the equation . The augmented matrix should be of reduced rank, and you can deduce the eigenvectors from the output. Check that an eigenvector you found is an eigenvector with eigenvalue by direct multiplication. Modify the code to output the condition number of a matrix. The condition number is the ratio of the largest to smallest eigenvalue. This controls how numerically stable inversion of the matrix is. ⎛ ⎝ ⎜ ⎜ ⎜ 1 −1 −1 −1 −1 2 0 0 −1 0 3 1 −1 0 1 4 ⎞ ⎠ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜⎜⎜⎜ 1 0 0 1 0 1 0 1/2 0 0 1 1/4 1 1/2 1/4 1 ⎞ ⎠ ⎟⎟⎟⎟ (A − I)x = 0λi λi SUMMARY AND FURTHER READINGSUMMARY AND FURTHER READING You should be reading additional material to provide a solid background to what we do in class All the textbooks contain sections on solving linear equations, for instance chapter 2 of . Numerical Recipes (http://www.nrbook.com/a/bookcpdf.php) SNAKESNAKE Use the arrow keys start gameJSXGraph v1.4.6 Copyright (C) see https://jsxgraph.org2 – o + ← ↓ ↑ →","libVersion":"0.5.0","langs":""}