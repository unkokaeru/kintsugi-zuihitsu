{"path":"attached/files/Session_1_Curve_Fitting_1.pdf","text":"NUMERICAL METHODS WEEK 1NUMERICAL METHODS WEEK 1 CURVE FITTING 0CURVE FITTING 0 OR SCIENTIFIC COMPUTING REFRESHEROR SCIENTIFIC COMPUTING REFRESHER This introduces the ideas of curve \u0000tting - this week the simplest case of \u0000tting a line to data we expect to be linearly related. Learning outcomes: Revise some material from Scienti\u0000c Computing last year. Code a working version of linear regression using C++. Check your code works correctly, via an external reference. MATT WATKINS MWATKINS@LINCOLN.AC.UKMATT WATKINS MWATKINS@LINCOLN.AC.UK WHAT IS NUMERICAL METHODS?WHAT IS NUMERICAL METHODS? Using computers to solve numerical problems in applied mathematics and physics. WHAT IS NUMERICAL METHODS NOT?WHAT IS NUMERICAL METHODS NOT? More programming training. WHY AM I LEARNING THIS?WHY AM I LEARNING THIS? Numerical competency will be one of the major skills you can bring to the market place alongside soft and professional skills. PHILOSOPHYPHILOSOPHY Break down problems into small chunks. Use pen and paper and plan your work before attacking the keyboard. Test, test and test again. NO REALLY - TRY AND TEST AFTER EVERY SINGLE LINE YOU ADD.NO REALLY - TRY AND TEST AFTER EVERY SINGLE LINE YOU ADD. SAVE - ON ONEDRIVE IT WILL KEEP BACKUPS TOO.SAVE - ON ONEDRIVE IT WILL KEEP BACKUPS TOO. LEAST SQUARES REGRESSIONLEAST SQUARES REGRESSION suppose that you think a set of paired observations are related as where is the error, or residual, between the model and the observations. We think there is a linear relationship between and , but there is some error in the measurements. ( , ), ( , ), … , ( , )x0 y0 x1 y1 xn−1 yn−1 = + + eyi a0 a1xi e x y BEST FITBEST FIT given our assumption of a straightline the error at each point is given by So in some sense the some of the total errors would be given by the sum of the errors. We will take the sum of the squares of the errors as our error criterion. = + +yi a0 a1xi ei = − −ei yi a0 a1xi = = ( − −Sr ∑ i=0 n−1 e2 i ∑ i=0 n−1 yi a0 a1xi) 2 So in some sense the some of the total errors would be given by the sum of the errors. We will take the sum of the squares of the errors as our error criterion. = = ( − −Sr ∑ i=0 n−1 e2 i ∑ i=0 n−1 yi a0 a1xi) 2 OPTIMAL PARAMETERSOPTIMAL PARAMETERS If we look at our model we see that there are 2 parameters, and that control the slope and intercept of our model. It is a model, we are assuming that there is a linear relationship between and . We want to minimize the value of , so we differentiate with respect to our parameters where the summations go from to ( this is to agree with C style arrays) . Note that the points are not variables, they are things we have measured. What we can vary is the parameters of our model. So is a function of the two parameters and . For more general models we will have more parameters and a more complex relations ship than the straight line assumed here. = = ( − −Sr ∑ i=0 n−1 e2 i ∑ i=0 n−1 yi a0 a1xi) 2 a0 a1 x y Sr ∂Sr ∂a0 ∂Sr ∂a1 = −2 ∑( − − )yi a0 a1xi = −2 ∑[( − − ) ]yi a0 a1xi xi 0 n − 1 ( , )xi yi Sr a0 a1 OPTIMAL PARAMETERSOPTIMAL PARAMETERS We want to minimize the value of , so we differentiate with respect to our parameters This gives us a pair of simultaneous linear equations, sometimes called the normal equations. We can solve these for and . and plug this into the \u0000rst equation to get where and are the means of the and values. Sr ∂Sr ∂a0 ∂Sr ∂a1 = −2 ∑( − − ) = 0yi a0 a1xi = −2 ∑[( − − ) ] = 0yi a0 a1xi xi a1 a0 =a1 n ∑ − ∑ ∑xiyi xi yi n ∑ − (∑x2 i xi)2 (1) = − = −a0 ∑ yi n a1 ∑ xi n y¯ a1x¯ (2) y¯ x¯ x y CODING UP LINEAR REGRESSIONCODING UP LINEAR REGRESSION You will want to use arrays to store data. Remember arrays are like a list, or ordered set, of numbers. The type of number is de\u0000ned in the normal way. Here is some code to allocate an array of size 1 0 0 , place the numbers 0 to 9 9 , in that order, into the array. Then we add up the elements of the array, and print them out. / * C + + c o d e * / # i n c l u d e < i o s t r e a m > u s i n g n a m e s p a c e s t d ; i n t m a i n ( ) { d o u b l e x [ 1 0 0 ] ; f o r ( i n t i = 0 ; i < 1 0 0 ; i + + ) { x [ i ] = i ; } d o u b l e s u m x = 0 . 0 ; f o r ( i n t i = 0 ; i < 1 0 0 ; i + + ) { s u m x + = x [ i ] ; } c o u t < < \" T h e s u m o f t h e n u m b e r s 0 t o 9 9 i s \" < < s u m x < < \" \\ n \" ; } h i g h l i g h t : c + + h l j s c p p # p y t h o n c o d e # c r e a t e a n e m p t y a r r a y x = [ ] f o r i i n r a n g e ( 1 0 0 ) : x . a p p e n d ( i ) s u m x = 0 f o r i i n r a n g e ( l e n ( x ) ) : # r a n g e ( n ) c o m m a n d c r e a t e s a l i s t o f v a l u e s f r o m 0 t o n - 1 s u m x + = i # p r i n t ( i ) p r i n t ( \" t h e s u m o f t h e n u m b e r s 0 t o 9 9 i s \" + s t r ( s u m x ) ) # s t r ( s u m x ) c o n v e r t s s u m x i n t o a s t r i n g h i g h l i g h t : p y t h o n h l j s EXERCISESEXERCISES Alter the previous code to answer the following questions: What is What is What is What is 2∑99 n=0 n2 n∑100 n=1 2n∑200 n=2 2∑99 n=0 n2 EXERCISESEXERCISES Find the intercept ( ) and slope ( ) of the least squares best \u0000t to the following data using the formulae given a few slides previously: a0 a1 x = [ 0 . 5 2 6 9 9 3 9 9 4 , 0 . 6 9 1 1 2 6 8 5 2 , 0 . 7 4 5 4 0 7 9 5 5 , 0 . 6 6 9 3 4 4 5 1 2 , 0 . 5 1 8 1 6 8 7 4 8 , 0 . 2 9 1 5 5 8 8 6 2 , 0 . 0 1 0 8 7 0 4 5 3 , 0 . 7 1 8 1 8 5 7 3 , 0 . 8 9 7 1 9 0 9 5 4 , 0 . 4 7 6 7 8 9 1 0 2 , ] y = [ 3 . 4 7 7 9 8 2 9 7 5 , 4 . 1 9 7 9 2 5 3 7 4 , 4 . 1 2 7 0 8 0 8 1 5 , 3 . 3 6 5 7 1 9 1 7 9 , 3 . 3 8 7 0 6 0 0 8 4 , 1 . 8 2 9 0 9 9 4 3 6 , 0 . 6 5 8 1 3 7 2 4 9 , 4 . 0 2 3 1 6 4 6 1 2 , 5 . 0 7 4 0 8 8 8 6 9 , 2 . 7 5 2 8 9 0 0 3 3 , ] TEST YOURSELFTEST YOURSELF That is it for the lecture! The really important thing this week is that you get a computer setup so that you can try the problems as we go forward. If you have problems getting the software to work on your laptop or desktop let us know.","libVersion":"0.5.0","langs":""}