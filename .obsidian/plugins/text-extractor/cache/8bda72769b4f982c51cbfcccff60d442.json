{"path":"attached/files/Session_8_Nonlinear_Equations.pdf","text":"NUMERICAL METHODS WEEK 7NUMERICAL METHODS WEEK 7 SOLUTIONS TO NONLINEAR EQUATIONSSOLUTIONS TO NONLINEAR EQUATIONS Learning outcomes: Understand and use Root Finding algorithms to solve nonlinear equations. Understand optimization problems. MATT WATKINS MWATKINS@LINCOLN.AC.UKMATT WATKINS MWATKINS@LINCOLN.AC.UK NON-LINEAR EQUATIONSNON-LINEAR EQUATIONS These are all the equations that are not linear. A linear function satis\u0000es and which imply the superposition principle An equation written is linear if is a linear function. Otherwise it is a nonlinear equation. The operations that map to can be very general, including, for instance differentiation. f(x + y) = f(x) + f(y) f(αx) = αf(x) f(αx + βy) = αf(x) + βf(y) f(x) = C f(x) + x − 1 = 0x2 x f(x) ROOT FINDING ALGORITHMSROOT FINDING ALGORITHMS In the simplest case we want to \u0000nd the solutions of In a few cases this can be done analytically - but generally we want an algorithm that will converge to a root. f(x) − C = 0 iterative OPTIMIZATIONOPTIMIZATION If we want to \u0000nd minima (or maxima) of a function f(x) we need to \u0000nd solutions of This is clearly the same problem as before - but we are now working with the derivative of the function. (x) = 0f ′ ROOT FINDING ALGORITHSROOT FINDING ALGORITHS There are many algorithms, often small modi\u0000cations of each other. Most well known are Bisection Newton-Raphson Secant Others are often combinations of the previous, aiming to keep their strengths but mitigate weaknesses. For instance the Brent algorithm is a combination of Bisection, Secant and interpolations. BISECTIONBISECTION Simplest possible algorithm. Let be a continuous function. Find an interval such that and have opposite signs (a bracket). Let be the middle of the interval. Then either and , or and have opposite signs, and one has divided by two the size of the interval. Rinse and repeat Although the bisection method is robust, it gains one and only one bit of accuracy with each iteration. Other methods, under appropriate conditions, can gain accuracy faster. f [a, b] f(a) f(b) c = (a + b)/2 f(a) f(c) f(c) f(b) NEWTON'S METHODNEWTON'S METHOD This looks just like our . If we have a continuous curve the equation of the tangent to the curve at the point is: If we want an estimate of the value where this cuts the -axis we look for the value of that satis\u0000es Which implies that linear interpolation (https://mattatlincoln.github.io/teaching/numerical_methods/lecture_4 /#/5 ) y = f(x) xn y = ( ) (x − ) + f( ),f ′ xn xn xn x x = xn+1 0 = ( ) ( − ) + f( )f ′ xn xn+1 xn xn = −xn+1 xn f( )xn ( )f ′ xn NEWTON'S METHODNEWTON'S METHOD Animation taken from wikipedia (https://en.wikipedia.org/wiki/Newton%2 7 s_method) ROOT FINDINGROOT FINDING Implement Newton's Method and use it to \u0000nd the following. Find solutions of Find solutions of = 612x2 = cos xx3 SECANT METHODSECANT METHOD The Secant method is essentially the same as Newton's method - but instead of calculating analytically, it is approximated by \u0000nite difference Plugging into Newton's method we get Note that you need to choose two (close) starting guesses, and before you can iterate the algorithm. (x)f ′ (x) ≈f ′ f(x+Δx)−f(x) (x+Δx)−x = − f( ) =xn xn−1 xn−1 −xn−1 xn−2 f( ) − f( )xn−1 xn−2 f( ) − f( )xn−2 xn−1 xn−1 xn−2 f( ) − f( )xn−1 xn−2 x0 x1 WHAT IS THE POINT?WHAT IS THE POINT? Sometimes it is dif\u0000cult or expensive to evaluate Related 'quasi-Newton' methods are very common for multidimensional optimization. Repeat the previous questions using the Secant method. How do the number of iterations compare? (x)f ′ OPTIMIZATIONOPTIMIZATION Newton's method for optimization is found by minimizing the quadratic approximation where is the estimated value of and we are currently at . Derive Newton's Method for optimization by \u0000nding the value of (call it ) that minimizes Implement Newton's Method for optimization and use it to \u0000nd minima of the following functions. , q( ) = f( ) + ( ) ⋅ ( − ) + ( ) ⋅ ( −xk+1 xk f ′ xk xk+1 xk 1 2 f ′′ xk xk+1 xk) 2 q( )xk+1 f( )xk+1 xk x x(k+1) q(x) f(x) = − cos xx2 f(x) = − 14 + 60 − 70xx4 x3 x2 ROOT FINDING IN HIGHER DIMENSIONSROOT FINDING IN HIGHER DIMENSIONS Newton's method or modi\u0000ed Newton schemes can be readily extended to higher dimensions. One may also use Newton's method to solve systems of (nonlinear) equations, which amounts to \u0000nding the zeroes of continuously differentiable functions - i.e. a vector is mapped to a vector. So we have and for each we have . k F : →Rk Rk = ( , + … + )xT x0 x1 xk−1 x F(x = ( , + … + )) T F0 F1 Fk−1 So instead of our linear approximation for the 1 D case we have a similar equation for each component of, We can write these equations in matrix form: This is just a set of equations of the form which we can solve to \u0000nd . Then we \u0000nd our new vector as . f(x + Δx) = f(x) + (x) (Δx) = 0,f ′ Fi (x + Δx) = (x) + Δ = 0,Fi Fi ∑ i=0 i=k−1 δ (x)Fi δxi xi k = + ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ (x + Δx)F0(x + Δx)F1 ⋮ (x + Δx)Fk−1 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ (x)F0(x)F1 ⋮ (x)Fk−1 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ δ (x)F0 δx0 δ (x)F1 δx0 ⋮ δ (x)Fk−1 δx0 δ (x)F0 δx1 δ (x)F1 δx1 ⋮ δ (x)Fk−1 δx1 … … ⋮ … δ (x)F0 δxk−1 δ (x)F1 δxk−1 ⋮ δ (x)Fk−1 δxk−1 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ Δx0 Δx1 ⋮ Δxk−1 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ = ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ 0 0 ⋮ 0 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ Ax = b Δx x x + Δx The matrix of derivatives is called the Jacobian, . You can also solve the equations by \u0000nding the inverse of . J(x) JOPTIMIZATION IN HIGHER DIMENSIONSOPTIMIZATION IN HIGHER DIMENSIONS Newton's method or modi\u0000ed Newton schemes for optimization can be readily extended to higher dimensions. You can \u0000nd minima of a scalar \u0000eld, ,using the same multidimensional Newton algorithm. If you have a scalar \u0000eld then is a series of equations that should be equal be equal to zero at a minimum (extrema). You can then use the algorithm on the previous slide to \u0000nd the zeros of the grad of . The Jacobian of the grad of a scalar \u0000eld is often called the Hessian and contains the mixed 2 nd order partial derivatives of the \u0000eld. f(x) f( , , … , )x1 x2 xn grad(f(x)) = ∇f(x) f = − ( ∇f( )xn+1 xn J∇f xn) −1 xn so to minimize we have to solvef(x) = ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ f(x)δ 2 δ δx0 x0 f(x)δ 2 δ δx1 x0 ⋮ f(x)δ 2 δ δxk−1 x0 f(x)δ 2 δ δx0 x1 f(x)δ 2 δ δx1 x1 ⋮ f(x)δ 2 δ δxk−1 x1 … … ⋮ … f(x)δ 2 δ δx0 xk−1 f(x)δ 2 δ δx1 xk−1 ⋮ f(x)δ 2 δ δxk−1 xk−1 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ Δx0 Δx1 ⋮ Δxk−1 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ − δf(x) δx0 − δf(x) δx1 ⋮ − δf(x) δxk−1 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ SUMMARY AND FURTHER READINGSUMMARY AND FURTHER READING You should be reading additional material to provide a solid background to what we do in class All the textbooks contain sections on root \u0000nding and solving non-linear equations, for instance chapter 9 of .Numerical Recipes (http://www.nrbook.com/a/bookcpdf.php)","libVersion":"0.5.0","langs":""}