{"path":"attached/files/Session_6_Questions and applications.pdf","text":"NUMERICAL METHODS WEEK 6NUMERICAL METHODS WEEK 6 RECAPRECAP Learning outcomes: Practice using some of the methods we have covered so far. See how to easily and tidyly reuse code. MATT WATKINS MWATKINS@LINCOLN.AC.UKMATT WATKINS MWATKINS@LINCOLN.AC.UK REUSING CODEREUSING CODEYou may dislike having lots of code / routines copied around everytime we reuse something The solution is to other \u0000les or, generally, libraries. We can make our own library \u0000le by copying functions other than m a i n into a \u0000le called ' m y _ l i b r a r y . h ' include This is my example project. Right click on the project name in the pane to right. Then select p r o p e r t i e s . bolded A new window should have appeared, like this: Open the C / C + + then the G e n e r a l tab Then click on drop down button in the second column of the the 'Additional Include Directories' row and select edit You should see something like this: Click on the folder icon, and select the directory where you saved your 'my_library.h' \u0000le in POWER METHODPOWER METHOD This is a way of \u0000nding the eigenvector with the largest eigenvector Now I can add a #include line in the \u0000le and use any functions in the library . Start with a guess at the eigenvector (it must not be orthogonal to the true eigenvector). Update the eigenvector Find the largest eigenvector of using the power method. We basically used this when studying Markov Chains in year 1 . x0 =xi Axn−1 ||A ||xn−1 A = . ⎛ ⎝ ⎜ ⎜ ⎜ 1 −1 −1 −1 −1 2 0 0 −1 0 3 1 −1 0 1 4 ⎞ ⎠ ⎟ ⎟ ⎟ GENERAL LINEAR LEAST SQUARESGENERAL LINEAR LEAST SQUARES Simple linear, polynomial and multiple linear regression can be generalised to the following linear least- squares model are now not just coordinates but some prede\u0000ned functions of those positions are . The linear refers to the parameters , the s can be highly non-linear For instance. \u0000ts this model with and . Where is a single independent variable and is a prede\u0000ned constant. I'll supress the functional dependence notation as it gets too much. Remember the s are prede\u0000ned - only the s are optimized. ( , , , … ; , , , … ) = + + + ⋯ + + eyi a0 a1 a2 a3 an x0 x1 x2 x3 xn a0z0 a1z1 a2z2 amzm xi ( , , , … ), ( , , , … ), … , ( , , , … )z0 x0 x1 x2 x3 xn z1 x0 x1 x2 x3 xn zm x0 x1 x2 x3 xn m + 1 basis functions , , … ,a0 a1 am z y( , , ; ) = + cos(ω ) + sin(ω )a0 a1 a2 x0 a0 a1 x0 a2 x0 = 1, = cos(ω )z0 z1 x0 = sin(ω )z2 x0 ω ω z a GENERAL LINEAR LEAST SQUARES EXAMPLEGENERAL LINEAR LEAST SQUARES EXAMPLE Let's redo the quadratic \u0000t example using the general linear least squares formalism. The general expression ( , , , … ; , , , … ) = ( ) + ( ) + ( ) + ⋯ + ( ) +yi a0 a1 a2 a3 an x0 x1 x2 x3 xn a0z0 xi a1z1 xi a2z2 xi amzm xi becomes as we only have three parameters. Comparing to the formula for a quadratic \u0000t we see that , and ( , , ; , , , … ) = ( ) + ( ) + ( ) + eyi a0 a1 a2 x0 x1 x2 x3 xn a0z0 xi a1z1 xi a2z2 xi ( , , ; , , , … ) = + + + eyi a0 a1 a2 x0 x1 x2 x3 xn a0 a1xi a2x2 i ( ) = 1z0 xi ( ) =z1 xi xi ( ) =z2 xi x2 i We write the set of equations for the all the data points in matrix formn + 1 = + ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ y0 y1 ⋮ yn ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ ⎡ ⎣ ⎢ ⎢⎢ ⎢⎢ ( )z0 x0 ( )z0 x1 ⋮ ( )z0 xn ( )z1 x0 ( )z1 x1 ⋮ ( )z1 xn ⋯ ⋯ ⋱ ⋯ ( )zm x0 ( )zm x1 ⋮ ( )zm xn ⎤ ⎦ ⎥ ⎥⎥ ⎥⎥ ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ a0 a1 ⋮ am ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ ⎡ ⎣ ⎢⎢⎢⎢⎢ e( )x0 e( )x1 ⋮ e( )xn ⎤ ⎦ ⎥⎥⎥⎥⎥ we'll de\u0000ne which contain the values of the basis functions and the errors at each point, and which contains the parameters we optimize. {Y } = , [Z] = , {e} = ⎡ ⎣ ⎢⎢⎢⎢⎢ y( )x0 y( )x1 ⋮ y( )xn ⎤ ⎦ ⎥⎥⎥⎥⎥ ⎡ ⎣ ⎢ ⎢⎢ ⎢⎢ ( )z0 x0 ( )z0 x1 ⋮ ( )z0 xn ( )z1 x0 ( )z1 x1 ⋮ ( )z1 xn ⋯ ⋯ ⋱ ⋯ ( )zm x0 ( )zm x1 ⋮ ( )zm xn ⎤ ⎦ ⎥ ⎥⎥ ⎥⎥ ⎡ ⎣ ⎢⎢⎢⎢⎢ e( )x0 e( )x1 ⋮ e( )xn ⎤ ⎦ ⎥⎥⎥⎥⎥ {A} = ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ a0 a1 ⋮ am ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ m + 1 We can also express error in our model as a sum of the squares much like before: You can show by taking partial derivatives that is minimised when =Sr ∑ i=0 n ( − ( ))yi ∑ j=0 m ajzj xi 2 Sr More details can of the derivation can be found , though the notation is a little different. [[Z [Z]]{A} = {[ ]{Y }}]T Z T here (http://fourier.eng.hmc.edu/e1 7 6 /lectures/NM/node3 5 .html) POWER METHOD, WHY DOES IT WORK?POWER METHOD, WHY DOES IT WORK? If our matrix is diagonalizable it has a set of orthogonal eigenvectors with eigenvalues with for . Any vector can be expanded as a linear combination the : , , … ,ν1 ν2 νn , , … ,λ1 λ2 λn | | > | |λ1 λj j > 1 νi = + + ⋯ + .b0 c1v1 c2v2 cmvm If we apply our matrix to times we get using the fact that for the eigenvectors . Because is larger than any other , all the terms except the \u0000rst in the bracket tend to zero as gets larger. A b0 k Akb0 = = = + + ⋯ +c1Akv1 c2Akv2 cmAkvm + + ⋯ +c1λk 1v1 c2λk 2v2 cmλk mvm ( + + ⋯ + ) .c1λk 1 v1 c2 c1 ( ) λ2 λ1 kv2 cm c1 ( ) λm λ1 kvm A =νi λiνi λ1 λ k SUMMARY AND FURTHER READINGSUMMARY AND FURTHER READING Ensure that you can do the material tested today - make and multiply matrices, solve linear equations - we will be using these repeatedly in the coming weeks. You should be reading additional material to provide a solid background to what we do in class I suggest starting with Chapra and Canale, but there are many other text books in the library - \u0000nd the one that works for you. SNAKESNAKE Use the arrow keys start gameJSXGraph v1.4.6 Copyright (C) see https://jsxgraph.org","libVersion":"0.5.0","langs":""}